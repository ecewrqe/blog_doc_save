目的：从各个门户网站爬重要的数据，提取到本地，进行分析加工
python做爬虫的几个模块：
requests
beautifulSoup4



爬github为例
```
import requests

from bs4 import BeautifulSoup
rs = requests.get("https://github.com/settings/emails")   #rs是一个response对象，封装了请求头和请求体
dic1=rs.cookies.get_dict()
dic1.text   #请求体的字符串格式
```
拿到text后解析，用BeautifulSoul4
```
obj = BeautifulSoup(rs.text,'html.parser')   #第二个参数传一个解析的模块

ul_obj=obj.find(name='ul',attrs={"id":"settings-emails"})
print(ul_obj)
```

对于用户登陆的情况：
第一次get请求拿到页面，第二次post请求拿到数据，如果通过，第三次自动get到某个页面。
这个过程浏览器进行三次请求，登陆的cookie可以设置在第一次请求，说明在第二次post必须带着这个cookie。也可以只是在第二次post的时候给cookie，这样第一次就不需要cookie。
第一次cookie的本质上是已经有cookie了，只是要对它授权
### 微信网页版登陆分析：
微信第一次get登陆，有几个返回的cookie："mm_lang=zh_CN; Domain=wx.qq.com; Path=/; Expires=Mon, 16-Oct-2017 13:13:56 GMT"
返回的内容为一段网页

一次网页登陆微信称为一次会话

为了拿到二维码图片，又去请求了一遍`https://login.wx.qq.com/jslogin` url中传值：
```
appid=wx782c26e4c19acffb
redirect_uri=https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxnewloginpage
fun=new
lang=zh_CN
_=1508170870883
```
返回值没有任何cookie
```
window.QRLogin.code = 200;    第一段为代号，说明是请求了一个uuid
window.QRLogin.uuid = "oeDHHW6S2w==";   #拿到会话id
```
在图片的位置，再次请求`https://login.weixin.qq.com/qrcode/oeDHHW6S2w==`会获取当前会话的二维码，此时可以扫码登陆。
但是我们不知道用户什么时候扫码，也不知道扫码完后什么时候确认登陆。
通过轮询，每次去请求`https://wx.qq.com/cgi-bin/mmwebwx-bin/login`，参数：
```
loginicon="true",
uuid=oeDHHW6S2w==,
tip=1,
r=-637696725,
_=1508170870898,


r = ~new Date
_ = +new Date
```
记住：request的params不会转换成json格式 ，也就是说，True不会变成"true"，**所以params中该是什么值就是什么值，该加引号的必须加引号**

如果loginicon的值不是"true"，扫码后是不会给图片的

每次轮询时间戳加一，会发现：轮询的时候会停住，那是微信的长轮询策略：当还没有扫码的时候，请求过去，服务器等待一段时间，判断是否有扫码，如果扫码成功立刻返回，或者超时后也返回。
超时后的返回结果是`window.code=408;`，扫码成功后的返回结果是`window.code=201;window.userAvatar = 'data:img/jpg;base64,/9j/4...;`。
**window.userAvatar** 的内容是用户的头像，只要再次请求就能拿到头像图片。




确认登陆后的返回结果是
```
window.code=200;
window.redirect_uri="https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxnewloginpage?ticket=AQopDSBuioYR9IkzeTs0gLVB@qrticket_0&uuid=oeDHHW6S2w==&lang=zh_CN&scan=1508171348";

```

然后去请求`https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxnewloginpage`：
**要注意的是，不是直接转跳上面的内容**
```
ticket=AQopDSBuioYR9IkzeTs0gLVB@qrticket_0
uuid=oeDHHW6S2w==
lang=zh_CN
scan=1508171348"
fun=new
version=v2
lang=zh_CN
```
拿到的内容：  **这里的内容很重要**  需要关注的是skey，wxsid，wxuin
```
<error>
	<ret>0</ret>
	<message></message>
	<skey>@crypt_fa479f5b_87540d98e80abbcb8aaf1e217e5ce7a1</skey>
	<wxsid>bfrc9b5JwGvCSczx</wxsid>
	<wxuin>1721825124</wxuin>
	<pass_ticket>
		1Zq90sZnIcRAyi7LaPqaD/Dx/ZjyJfoanRA57ln36LshRkdxa7lnZj3LcgAl/6Fy
	</pass_ticket>
	<isgrayscale>1</isgrayscale>
</error>
```

然后POST请求`https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxinit`
```
#url携带
r=-637350054
lang=zh_CN
pass_ticket=1Zq90sZnIcRAyi7LaPqaD%252FDx%252FZjyJfoanRA57ln36LshRkdxa7lnZj3LcgAl%252F6Fy
# 请求内容：
DeviceID:"e173209395813376"
Sid:"bfrc9b5JwGvCSczx"
Skey:"@crypt_fa479f5b_87540d98e80abbcb8aaf1e217e5ce7a1"
Uin:"1721825124"


DeviceID是一串随机数，生成源码是：
function getDeviceID () {
	return "e" + ("" + Math.random().toFixed(15)).substring(2, 17);
}
```

[具体json](/static/lib/wechat_init.json "具体json" target="ddd")
该拿到的是部分好友信息

要拿到全部好友信息,POST请求
`https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxgetcontact?`
```
lang=zh_CN
pass_ticket=rD2Y04YCjDd1k7bQ6%252BwDCRXtjksRiUaq9qvedvUmrBWlSukZax%252BZALC1MzYJWT4D
r=1508201450199
seq=0
skey=@crypt_fa479f5b_5343a02fe2cb875b5031b039c0118032
```
[具体json](/static/lib/wechat_new.json )

**refer**



request.get参数：
url
params  传一个字典给url,作为get的参数
stream=True   当需要下载的文件很大，访问又慢，开启这个参数，循环接收内容

data  传请求头，只在post和put用
json   xxxxxxxxx，dump后的json格式
cookies  携带cookies
timeout 连接超时和等待超时
files  上传文件
allow_redirecst=true  是否自动重定向
proxies={}   代理

data发送application/x-www-form-urlencoded数据格式
files发送multipart/form-data格式
json发送application/json格式


Session 保存cookie
```
session = requests.Session()
```

response的属性：
response.status_code
response.cookies.get_dict()
response.content   字节形式的请求体
response.text   字符串形式的请求体，如果乱码，解决方法是在之前更改response.encoding，或者直接使用content转
response.header   响应头

response.iter_content   当需要下载文件时，get参数开启stream的时候，这边会循环接受
```
resp=requests.get("",stream=True)
for content in resp.iter_content:
    pass
```

bs4obj：
解析出来的内容就像dom对象，直接通过find找标签，大致方法
obj.find
obj.findall(name,recursive=False，text=xxx)


obj.text   取标签内容
obj.attrs  取所有标签属性
obj.get   取某一个标签的属性值


obj.children  下一级对象的迭代器
obj.clear()    清空所有标签的内容
obj.decompose()
obj.encode()   变成字节
obj.decode()   变成字符串

obj.is_empty_element()
obj.select()

**轮询策略**
每秒向服务器发送

****

### http工具库
httplib
```
"""
connect：
HTTPConnection, HTTPSConnection

请求
conn.request(method, url, body, headers)

返回结果
conn.getresponse() -> r1
r1.status -- 200
r1.reason -- OK
r1.version -- 11
r1.length -- 14615
r1.msg
    Date: Tue, 27 Feb 2018 08:14:46 GMT
    Content-Type: text/html
    Content-Length: 14615
    ...

r1.getheaders() --- [('content-length', '14615'), ('set-cookie', 'BAIDUID=...)]
r1.getheader("set-cookie") --- BAIDUID=2C7226BBA473FEC38619D06489...

"""
```
urllib.urlencode()
requests发送数据是直接发送json格式的，http的发送body的默认格式和url传送的格式
`a=10&b=20`  这个格式是application/x-www-form-urlencoded格式
urllib.urlencode就是把python的字典转换成x-www-form-urlencoded格式
```
params = {"aa": "bb", "cc": "dd"}
params = urllib.urlencode(params)
print(params)
--
  aa=bb&cc=dd
params = {"aa": "bb", "cc": {"dd": "ee"}}
params = urllib.urlencode(params)
print(params)
--
  aa=bb&cc=%7Bu%27dd%27%3A+u%27ee%27%7D
```
解析和反解析http字节
```
>>> import urllib
>>> a = "hai=<>%?fa"
>>> a = urllib.quote(a)
>>> print(a)
hai%3D%3C%3E%25%3Ffa
>>> urllib.unquote(a)
'hai=<>%?fa'
```
quote,unquote,quote_plus,unquote_plus


urlparse库
```
url = "http://user:pass@www.example.org:88/aa/bb.html?arm=close&leg=open&mouse=fuck#first"
up = urlparse.urlparse(url)
print(up)
--
  ParseResult(scheme=u'http', netloc=u'user:pass@www.example.org:88', path=u'/aa/bb.html', params='', query=u'arm=close&leg=open&mouse=fuck', fragment=u'first')
######
```
拆分x-www-form-urlencoded为python字典
```
print(up.query)
arm=close&leg=open&mouse=fuck
```
parse_qs和parse_qsl
```
url = "http://user:pass@www.example.org:88/aa/bb.html?arm=caaa&arm=close&leg=open&mouse=fuck#first"
query = urlparse.parse_qs(up.query)
--
  query:{u'mouse': [u'fuck'], u'arm': [u'caaa', u'close'], u'leg': [u'open']}
#####
query = urlparse.parse_qsl(up.query)
--
  [(u'arm', u'caaa'), (u'arm', u'close'), (u'leg', u'open'), (u'mouse', u'fuck')]
```

### 协程和异步非阻塞
协程是让一个线程，在多个任务之间切换，yield就是协程。





















