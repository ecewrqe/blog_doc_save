目的：从各个门户网站爬重要的数据，提取到本地，进行分析加工
python做爬虫的几个模块：
requests
beautifulSoul4

爬github为例
```
import requests

from bs4 import BeautifulSoup
rs = requests.get("https://github.com/settings/emails")   #rs是一个response对象，封装了请求头和请求体
dic1=rs.cookies.get_dict()
dic1.text   #请求体的字符串格式
```
拿到text后解析，用BeautifulSoul4
```
obj = BeautifulSoup(rs.text,'html.parser')   #第二个参数传一个解析的模块

ul_obj=obj.find(name='ul',attrs={"id":"settings-emails"})
print(ul_obj)
```

对于用户登陆的情况：
第一次get请求拿到页面，第二次post请求拿到数据，如果通过，第三次自动get到某个页面。
这个过程浏览器进行三次请求，登陆的cookie可以设置在第一次请求，说明在第二次post必须带着这个cookie。也可以只是在第二次post的时候给cookie，这样第一次就不需要cookie。








request.get参数：
url
params  传一个字典给url,作为get的参数
stream=True   当需要下载的文件很大，访问又慢，开启这个参数，循环接收内容

data  传请求头，只在post和put用
json   xxxxxxxxx，dump后的json格式
cookies  携带cookies
timeout 连接超时和等待超时
files  上传文件
allow_redirecst=true  是否自动重定向
proxies={}   代理

Session 保存cookie
```
session = requests.Session()
```

response的属性：
response.status_code
response.cookies.get_dict()
response.content   字节形式的请求体
response.text   字符串形式的请求体，如果乱码，解决方法是在之前更改response.encoding，或者直接使用content转
response.header   响应头

response.iter_content   当需要下载文件时，get参数开启stream的时候，这边会循环接受
```
resp=requests.get("",stream=True)
for content in resp.iter_content:
    pass
```

bs4obj：
解析出来的内容就像dom对象，直接通过find找标签，大致方法
obj.find
obj.findall(name,recursive=False，text=xxx)


obj.text   取标签内容
obj.attrs  取所有标签属性
obj.get   取某一个标签的属性值


obj.children  下一级对象的迭代器
obj.clear()    清空所有标签的内容
obj.decompose()
obj.encode()   变成字节
obj.decode()   变成字符串

obj.is_empty_element()
obj.select()











**轮询策略**
每秒向服务器发送

****
















### 协程和异步非阻塞
协程是让一个线程，在多个任务之间切换，yield就是协程。















