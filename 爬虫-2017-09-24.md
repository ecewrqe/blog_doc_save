目的：从各个门户网站爬重要的数据，提取到本地，进行分析加工
python做爬虫的几个模块：
requests
beautifulSoul4

爬github为例
```
import requests

from bs4 import BeautifulSoup
rs = requests.get("https://github.com/settings/emails")   #rs是一个response对象，封装了请求头和请求体
dic1=rs.cookies.get_dict()
dic1.text   #请求体的字符串格式
```
拿到text后解析，用BeautifulSoul4
```
obj = BeautifulSoup(rs.text,'html.parser')   #第二个参数传一个解析的模块

ul_obj=obj.find(name='ul',attrs={"id":"settings-emails"})
print(ul_obj)
```

浏览器第一次发送请求后，可能会收到cookie，当登陆的时候要带着这个cookie去，






request.get参数：
url
params  传一个字典给url,作为get的参数
stream=True   当需要下载的文件很大，访问又慢，开启这个参数，循环接收内容

data  传请求头，只在post和put用
cookies  携带cookies

response的属性：
response.status_code
response.cookies.get_dict()
response.content   字节形式的请求体
response.text   字符串形式的请求体，如果乱码，解决方法是在之前更改response.encoding，或者直接使用content转
response.header   响应头

response.iter_content   当需要下载文件时，get参数开启stream的时候，这边会循环接受

bs4obj：
解析出来的内容就像dom对象，直接通过find找标签，大致方法
obj.find
obj.findall
obj.text   取标签内容
obj.attrs  取所有标签属性
obj.get   取某一个标签的值

obj.children  下一级对象的迭代器
















